{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fc2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from exp.exp_informer import Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c808af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyats.datastructures import AttrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d6e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AttrDict({\"activation\": 'gelu', \"attn\": 'prob', \"batch_size\": 32, \n",
    "        \"c_out\": 7, \"checkpoints\": './checkpoints/', \"cols\": None, \n",
    "        \"d_ff\": 2048, \"d_layers\": 1, \"d_model\": 320, \"data\": 'ETTh1', \n",
    "        \"data_path\": 'ETTh1.csv', \"dec_in\": 7, \"des\": 'Exp', \"detail_freq\": 'h', \n",
    "        \"devices\": '0,1,2,3', \"distil\": True, \"do_predict\": False, \"dropout\": 0.05, \n",
    "        \"e_layers\": 5, \"embed\": 'timeF', \"enc_in\": 7, \"factor\": 5, \"features\": 'M', \"freq\": 'h', \n",
    "        \"gpu\": 0, \"inverse\": False, \"itr\": 1, \"label_len\": 48, \"learning_rate\": 0.0001, \"loss\": 'mse', \n",
    "        \"lradj\": 'type1', \"mix\": True, \"model\": 'informer-moco', \"n_heads\": 8, \"num_workers\": 0, \n",
    "        \"output_attention\": False, \"padding\": 0, \"patience\": 3, \"pred_len\": 24, \"root_path\": './data/ETT/', \n",
    "        \"s_layers\": [3, 2, 1], \"seq_len\": 48, \"target\": 'OT', \"train_epochs\": 1, \"use_amp\": False, \n",
    "        \"use_gpu\": True, \"use_multi_gpu\": False, \"loss_lambda\": 0.0, \"mask_rate\": 0.2, \"kernel_size\": 3, \"l2norm\": True,\n",
    "                \"moco_average_pool\": False, \"data_aug\": \"cost\", \"mare\": True, \"time_feature_embed\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507199a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bbe18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69514dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "AttrDict({'activation': 'gelu', 'attn': 'prob', 'batch_size': 32, 'c_out': 7, 'checkpoints': './checkpoints/', 'cols': None, 'd_ff': 2048, 'd_layers': 1, 'd_model': 320, 'data': 'ETTh1', 'data_path': 'ETTh1.csv', 'dec_in': 7, 'des': 'Exp', 'detail_freq': 'h', 'devices': '0,1,2,3', 'distil': True, 'do_predict': False, 'dropout': 0.05, 'e_layers': 5, 'embed': 'timeF', 'enc_in': 7, 'factor': 5, 'features': 'M', 'freq': 'h', 'gpu': 0, 'inverse': False, 'itr': 1, 'label_len': 48, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'mix': True, 'model': 'informer-moco', 'n_heads': 8, 'num_workers': 0, 'output_attention': False, 'padding': 0, 'patience': 3, 'pred_len': 24, 'root_path': './data/ETT/', 's_layers': [3, 2, 1], 'seq_len': 48, 'target': 'OT', 'train_epochs': 1, 'use_amp': False, 'use_gpu': False, 'use_multi_gpu': False, 'loss_lambda': 0.0, 'mask_rate': 0.2, 'kernel_size': 3, 'l2norm': True, 'moco_average_pool': False, 'data_aug': 'cost', 'mare': False, 'time_feature_embed': False})\n"
     ]
    }
   ],
   "source": [
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'WTH':{'data':'WTH.csv','T':'WetBulbCelsius','M':[12,12,12],'S':[1,1,1],'MS':[12,12,1]},\n",
    "    'ECL':{'data':'ECL.csv','T':'MT_320','M':[321,321,321],'S':[1,1,1],'MS':[321,321,1]},\n",
    "    'Solar':{'data':'solar_AL.csv','T':'POWER_136','M':[137,137,137],'S':[1,1,1],'MS':[137,137,1]},\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]\n",
    "\n",
    "# args.s_layers = [int(s_l) for s_l in args.s_layers.replace(' ','').split(',')]\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]\n",
    "args.time_feature_embed = False\n",
    "args.mare = False\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447b8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dtcn_encoder import *\n",
    "from models.tcn_encoder import *\n",
    "from models.lstm_encoder import LSTM_Encoder\n",
    "from models.informer_encoder import Informer_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a18a8327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = Informer_Encoder(320, d_model=128, factor=2, d_ff=64, e_layer=5, distil=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e0c739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457216\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78a8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([32, 128, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b447eff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      "Use Data augmentation method: cost\n",
      "l2norm True\n",
      "Mask Rate: 0.5\n",
      "[INFO] NOT Using Time Features.\n",
      "[INFO] Number of parameters:  768647\n",
      "TCN_MoCo(\n",
      "  (encoder_q): TCNBase(\n",
      "    (enc_embedding): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (encoder): Informer_Encoder(\n",
      "      (encoder): Encoder(\n",
      "        (attn_layers): ModuleList(\n",
      "          (0): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (1): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (2): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (3): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (4): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (projection): Linear(in_features=128, out_features=320, bias=True)\n",
      "    )\n",
      "    (drop): Dropout(p=0.05, inplace=False)\n",
      "  )\n",
      "  (encoder_k): TCNBase(\n",
      "    (enc_embedding): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (encoder): Informer_Encoder(\n",
      "      (encoder): Encoder(\n",
      "        (attn_layers): ModuleList(\n",
      "          (0): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (1): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (2): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (3): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (4): EncoderLayer(\n",
      "            (attention): AttentionLayer(\n",
      "              (inner_attention): ProbAttention(\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "              )\n",
      "              (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (conv1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
      "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (projection): Linear(in_features=128, out_features=320, bias=True)\n",
      "    )\n",
      "    (drop): Dropout(p=0.05, inplace=False)\n",
      "  )\n",
      "  (head_q): Sequential(\n",
      "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=320, out_features=320, bias=True)\n",
      "  )\n",
      "  (head_k): Sequential(\n",
      "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=320, out_features=320, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=320, out_features=7, bias=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.05, inplace=False)\n",
      ")\n",
      ">>>>>>>start training : informer-moco_ETTh1_ftM_sl48_ll48_pl24_dm320_nh8_el5_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 8569\n",
      "val 2857\n",
      "test 2857\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "\titers: 1, epoch: 1 | loss: 1.5543658\n",
      "\tspeed: 0.4414s/iter; left time: 117.8537s\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "\titers: 101, epoch: 1 | loss: 0.2773207\n",
      "\tspeed: 0.3985s/iter; left time: 66.5438s\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "\titers: 201, epoch: 1 | loss: 0.3234643\n",
      "\tspeed: 0.4577s/iter; left time: 30.6662s\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "Epoch: 1 cost time: 113.41088700294495\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "torch.Size([32, 320, 48])\n",
      "Epoch: 1, Steps: 267 | Train Loss: 0.3404900 Vali Loss: 0.6293078 Test Loss: 0.4632477\n",
      "Validation loss decreased (inf --> 0.629308).  Saving model ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AttrDict' object has no attribute 'cos_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m exp \u001b[38;5;241m=\u001b[39m Exp(args) \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n\u001b[0;32m---> 10\u001b[0m exp\u001b[38;5;241m.\u001b[39mtrain(setting)\n",
      "File \u001b[0;32m~/projects/code_test/informer/exp/exp_informer.py:443\u001b[0m, in \u001b[0;36mExp_Informer.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos_lr\u001b[49m:\n\u001b[1;32m    444\u001b[0m     adjust_learning_rate_cos(model_optim, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AttrDict' object has no attribute 'cos_lr'"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, \n",
    "                args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    exp = Exp(args) # set experiments\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(exp.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "16265749 16528385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb83f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TCN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f202e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 128, 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b43777",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn = TemporalConvNet(128, [512,512], dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm = nn.LSTM(128, 512, num_layers=3, batch_first=True, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc257ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 72, 128)\n",
    "out, (hn, cn) = lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Linear(512, 7, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a885c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6790c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TCN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from models.embed import TimeFeatureEmbedding\n",
    "import sys, math, random, copy\n",
    "from typing import Union, Callable, Optional, List\n",
    "from models.weight_norm import WeightNorm\n",
    "from models.data_aug import Data_Aug\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = WeightNorm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation), ['weight'])\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = WeightNorm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation), ['weight'])\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.module.weight_v.data.normal_(0, 0.01)\n",
    "        self.conv2.module.weight_v.data.normal_(0, 0.01)\n",
    "        self.conv1.module.weight_g.data.normal_(0, 0.01)\n",
    "        self.conv2.module.weight_g.data.normal_(0, 0.01)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e455380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_receptive_field.torch_receptive_field import receptive_field\n",
    "# from models.tcn_encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7f29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DilatedConvEncoder(64,  [64]*10 + [320], 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3713c813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DilatedConvEncoder(\n",
       "  (net): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "      )\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "      )\n",
       "    )\n",
       "    (4): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "      )\n",
       "    )\n",
       "    (5): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "      )\n",
       "    )\n",
       "    (6): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "      )\n",
       "    )\n",
       "    (7): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "    )\n",
       "    (8): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(256,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(256,))\n",
       "      )\n",
       "    )\n",
       "    (9): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(512,), dilation=(512,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(512,), dilation=(512,))\n",
       "      )\n",
       "    )\n",
       "    (10): ConvBlock(\n",
       "      (conv1): SamePadConv(\n",
       "        (conv): Conv1d(64, 320, kernel_size=(3,), stride=(1,), padding=(1024,), dilation=(1024,))\n",
       "      )\n",
       "      (conv2): SamePadConv(\n",
       "        (conv): Conv1d(320, 320, kernel_size=(3,), stride=(1,), padding=(1024,), dilation=(1024,))\n",
       "      )\n",
       "      (projector): Conv1d(64, 320, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d22ad295",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([4, 64, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01389f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TemporalBlock' object has no attribute 'TemporalBlock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m receptive_field_dict \u001b[38;5;241m=\u001b[39m receptive_field(encoder\u001b[38;5;241m.\u001b[39mnetwork[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mTemporalBlock, (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m96\u001b[39m))\n",
      "File \u001b[0;32m~/ts2vec/lib/python3.8/site-packages/torch/nn/modules/module.py:947\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 947\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TemporalBlock' object has no attribute 'TemporalBlock'"
     ]
    }
   ],
   "source": [
    "receptive_field_dict = receptive_field(encoder.network[0], (64, 96))\n",
    "# receptive_field_for_unit(receptive_field_dict, \"2\", (101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7159dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalConvNet\n",
      "Sequential\n",
      "TemporalBlock\n",
      "WeightNorm\n",
      "Conv1d\n"
     ]
    }
   ],
   "source": [
    "for name, module in encoder.named_modules():\n",
    "    print(str(module.__class__).split(\".\")[-1].split(\"'\")[0])\n",
    "    if str(module.__class__).split(\".\")[-1].split(\"'\")[0] == \"Conv1d\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf0ee4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb64730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(3, 6, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(6, 6, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.nettt = nn.Sequential(self.conv, self.conv2)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.nettt(x)\n",
    "\n",
    "class Net1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1D, self).__init__()\n",
    "        self.net = TemporalBlock()\n",
    "        self.bn = nn.BatchNorm1d(6)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        y = self.bn(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.maxpool(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e730228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(3, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "in Conv1d\n",
      "---------\n",
      "Conv1d(6, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "in Conv1d\n",
      "---------\n",
      "Conv1d(3, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "in Conv1d\n",
      "---------\n",
      "Conv1d(6, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "in Conv1d\n",
      "---------\n",
      "BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "in BatchNorm1d\n",
      "---------\n",
      "MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "in MaxPool1d\n",
      "---------\n",
      "6\n",
      "<torch.utils.hooks.RemovableHandle object at 0x152485ba5d30>\n",
      "Conv1d(3, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "class_name Conv1d\n",
      "Conv1d(3, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "class_name Conv1d\n",
      "Conv1d(6, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "class_name Conv1d\n",
      "Conv1d(6, 6, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "class_name Conv1d\n",
      "BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "class_name BatchNorm1d\n",
      "MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "class_name MaxPool1d\n",
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size      start       jump receptive_field \n",
      "==============================================================================\n",
      "        0                   [16]        0.5        1.0             1.0 \n",
      "        1                   [16]        0.5        1.0             3.0 \n",
      "        2                   [16]        0.5        1.0             5.0 \n",
      "        3                   [16]        0.5        1.0             7.0 \n",
      "        4                   [16]        0.5        1.0             9.0 \n",
      "        5                   [16]        0.5        1.0             9.0 \n",
      "        6                    [9]        0.0        2.0            10.0 \n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "model = Net1D()\n",
    "receptive_field_dict = receptive_field(model, (3, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec83687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "xx =OrderedDict([('0', OrderedDict([('j', 1.0), ('r', 1.0), ('start', 0.5), ('conv_stage', True), ('output_shape', [-1, 64, 96])])), ('1', OrderedDict([('input_shape', [-1, 64, 96]), ('output_shape', [-1, 64, 98])])), ('2', OrderedDict())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "076943fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99ca98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
